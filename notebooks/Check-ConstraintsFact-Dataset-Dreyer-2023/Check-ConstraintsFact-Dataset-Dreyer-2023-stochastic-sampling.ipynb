{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b1442b",
   "metadata": {},
   "source": [
    "# Reproducing codebase as Python API\n",
    "\n",
    "The notebook reproduces the trained-BART model as the Python API not as the CLI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f4c4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmitsuzawa/.local/miniconda3/envs/p39-Dreyer-2023/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "import typing as ty\n",
    "\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "from torch.multiprocessing import Pool, set_start_method\n",
    "set_start_method('spawn', force=True)\n",
    "from functools import partial\n",
    "import more_itertools as mit\n",
    "\n",
    "import torch\n",
    "import fairseq\n",
    "from fairseq.models.bart import BARTHubInterface\n",
    "from fairseq.models.bart import BARTModel\n",
    "\n",
    "import nvgpu\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb361a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logzero\n",
    "\n",
    "from datetime import datetime\n",
    "_datetime_exec = datetime.now()\n",
    "\n",
    "logzero.logfile(f\"logs/{_datetime_exec.isoformat()}.log\")\n",
    "\n",
    "logger = logzero.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977ed794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(task: Path, model_path: Path) -> BARTHubInterface:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        task: a path to the directory of the model.\n",
    "        model_path: a path to 'model.pt' file.\n",
    "    \"\"\"\n",
    "    assert task.exists()\n",
    "    assert model_path.exists()\n",
    "\n",
    "    logger.info(f\"Loading model {model_path}\")\n",
    "    model_dirname, model_fname = os.path.split(model_path.as_posix())\n",
    "    bart = BARTModel.from_pretrained(\n",
    "        model_dirname,\n",
    "        checkpoint_file=model_fname,\n",
    "        data_name_or_path=task.as_posix()\n",
    "    )\n",
    "    logger.info(f\"Loading done.\")\n",
    "    return bart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f758873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to input\n",
    "PATH_TEXT_FILE_INPUT = Path(\"/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/xsum/test_source.txt\")\n",
    "assert PATH_TEXT_FILE_INPUT.exists()\n",
    "\n",
    "seq_text_input = PATH_TEXT_FILE_INPUT.open().readlines()\n",
    "assert len(seq_text_input) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa15ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 250709 10:50:41 2610531437:10] Loading model /workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.cnn/model.pt\n"
     ]
    }
   ],
   "source": [
    "# with xsum model\n",
    "# PATH_MODEL_FILE = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.xsum')\n",
    "# with cnn model\n",
    "PATH_MODEL_FILE = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.cnn')\n",
    "\n",
    "bart_model = load_model(PATH_MODEL_FILE, PATH_MODEL_FILE / 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2046a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.models.bart.hub_interface.BARTHubInterface"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bart_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807ca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 250709 10:47:28 3849268264:1] BARTHubInterface(\n",
      "      (model): BARTModel(\n",
      "        (encoder): TransformerEncoder(\n",
      "          (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
      "          (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
      "          (layers): ModuleList(\n",
      "            (0): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): TransformerDecoder(\n",
      "          (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
      "          (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
      "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (layers): ModuleList(\n",
      "            (0): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): TransformerDecoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MultiheadAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (output_projection): Linear(in_features=1024, out_features=50264, bias=False)\n",
      "        )\n",
      "        (classification_heads): ModuleDict()\n",
      "      )\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "logger.info(str(bart_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_obj = torch.device('cuda:0')\n",
    "else:\n",
    "    device_obj = torch.device('cpu')\n",
    "# end if\n",
    "\n",
    "bart_model = bart_model.to(device_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dcc697",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.device' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m bart\u001b[38;5;241m.\u001b[39msample(batch, beam\u001b[38;5;241m=\u001b[39mbeam, lenpen\u001b[38;5;241m=\u001b[39mlenpen,\n\u001b[1;32m     12\u001b[0m                             min_len\u001b[38;5;241m=\u001b[39mmin_len, max_len_a\u001b[38;5;241m=\u001b[39mmax_len_a, max_len_b\u001b[38;5;241m=\u001b[39mmax_len_b,\n\u001b[1;32m     13\u001b[0m                             no_repeat_ngram_size\u001b[38;5;241m=\u001b[39mno_repeat_ngram_size,\n\u001b[1;32m     14\u001b[0m                             extractive_penalty_fct\u001b[38;5;241m=\u001b[39mextractive_penalty_fct)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# end def\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mbart_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbart_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_text_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextractive_penalty_fct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog_exp(2,2.402244)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mbart_sample\u001b[0;34m(bart, batch, extractive_penalty_fct, beam, lenpen, min_len, max_len_a, max_len_b, no_repeat_ngram_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbart_sample\u001b[39m(bart: BARTHubInterface,\n\u001b[1;32m      2\u001b[0m                 batch: ty\u001b[38;5;241m.\u001b[39mList[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m      3\u001b[0m                 extractive_penalty_fct: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 max_len_b: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m140\u001b[39m,\n\u001b[1;32m      9\u001b[0m                 no_repeat_ngram_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlenpen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlenpen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmin_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mextractive_penalty_fct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextractive_penalty_fct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/fairseq/fairseq/models/bart/hub_interface.py:106\u001b[0m, in \u001b[0;36mBARTHubInterface.sample\u001b[0;34m(self, sentences, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: List[\u001b[38;5;28mstr\u001b[39m], beam: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m--> 106\u001b[0m     hypos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m hypos]\n",
      "File \u001b[0;32m/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/fairseq/fairseq/models/bart/hub_interface.py:110\u001b[0m, in \u001b[0;36mBARTHubInterface.generate\u001b[0;34m(self, tokens, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[torch\u001b[38;5;241m.\u001b[39mLongTensor], beam: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor:\n\u001b[0;32m--> 110\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# build generator using current args as well as any kwargs\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     gen_args \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/fairseq/fairseq/models/bart/hub_interface.py:97\u001b[0m, in \u001b[0;36mBARTHubInterface._build_sample\u001b[0;34m(self, src_tokens)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_build_sample\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_tokens: List[torch\u001b[38;5;241m.\u001b[39mLongTensor]):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# assert torch.is_tensor(src_tokens)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39mbuild_dataset_for_inference(\n\u001b[1;32m     94\u001b[0m         src_tokens,\n\u001b[1;32m     95\u001b[0m         [x\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m src_tokens],\n\u001b[1;32m     96\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollater\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     sample \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mapply_to_sample(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m tensor: tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    100\u001b[0m         sample\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "File \u001b[0;32m/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/fairseq/fairseq/data/language_pair_dataset.py:258\u001b[0m, in \u001b[0;36mLanguagePairDataset.collater\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollater\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Merge a list of samples to form a mini-batch.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m              on the left if *left_pad_target* is ``True``.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_pad_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_pad_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_pad_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_pad_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_feeding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_feeding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/fairseq/fairseq/data/language_pair_dataset.py:57\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(samples, pad_idx, eos_idx, left_pad_source, left_pad_target, input_feeding)\u001b[0m\n\u001b[1;32m     55\u001b[0m src_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m samples])\n\u001b[1;32m     56\u001b[0m src_lengths, sort_order \u001b[38;5;241m=\u001b[39m src_lengths\u001b[38;5;241m.\u001b[39msort(descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 57\u001b[0m sort_order \u001b[38;5;241m=\u001b[39m sort_order\u001b[38;5;241m.\u001b[39mto(\u001b[43msrc_lengths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m0\u001b[39m, sort_order)\n\u001b[1;32m     59\u001b[0m src_tokens \u001b[38;5;241m=\u001b[39m src_tokens\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m0\u001b[39m, sort_order)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.device' object is not callable"
     ]
    }
   ],
   "source": [
    "def bart_sample(bart: BARTHubInterface,\n",
    "                batch: ty.List[str],\n",
    "                extractive_penalty_fct: str,\n",
    "                beam: int = 4,\n",
    "                lenpen: float = 2.0,  # length penalty\n",
    "                min_len: int = 55,\n",
    "                max_len_a: int = 0,\n",
    "                max_len_b: int = 140,\n",
    "                no_repeat_ngram_size: int = 3):\n",
    "    with torch.no_grad():\n",
    "        return bart.sample(batch, beam=beam, lenpen=lenpen,\n",
    "                            min_len=min_len, max_len_a=max_len_a, max_len_b=max_len_b,\n",
    "                            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                            extractive_penalty_fct=extractive_penalty_fct)\n",
    "# end def\n",
    "\n",
    "\n",
    "res = bart_sample(\n",
    "    bart=bart_model,\n",
    "    batch=seq_text_input,\n",
    "    extractive_penalty_fct='log_exp(2,2.402244)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d69c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_sample_stochastic(bart: BARTHubInterface,\n",
    "                           batch: ty.List[str],\n",
    "                           temperature: float,\n",
    "                           extractive_penalty_fct: str,\n",
    "                           random_seed_value: int,                           \n",
    "                           beam: int = 4,\n",
    "                           lenpen: float = 2.0,  # length penalty\n",
    "                           min_len: int = 55,\n",
    "                           max_len_a: int = 0,\n",
    "                           max_len_b: int = 140,\n",
    "                           no_repeat_ngram_size: int = 3):\n",
    "    dict_parameters = dict(\n",
    "        beam=beam,\n",
    "        lenpen=lenpen,\n",
    "        sampling=True,\n",
    "        temperature=temperature,\n",
    "        min_len=min_len, \n",
    "        max_len_a=max_len_a, \n",
    "        max_len_b=max_len_b,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        extractive_penalty_fct=extractive_penalty_fct)\n",
    "\n",
    "    with (torch.random.fork_rng(), torch.no_grad()):\n",
    "        torch.manual_seed(random_seed_value)\n",
    "        torch.cuda.manual_seed_all(random_seed_value)  # if you are using multi-GPU.        \n",
    "        res = bart.sample(batch, **dict_parameters)\n",
    "\n",
    "    return res\n",
    "# end def\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c00b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # case Xsum constraints dataset\n",
    "# import json\n",
    "\n",
    "# PATH_CONSTRAINS_XSUM = Path(\"/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/datasets/constraints_fact_v1.0/xsum/collect.json\")\n",
    "# assert PATH_CONSTRAINS_XSUM.exists()\n",
    "\n",
    "# with PATH_CONSTRAINS_XSUM.open() as f:\n",
    "#     seq_dataset = [json.loads(_line) for _line in f.readlines()]\n",
    "# # end with\n",
    "\n",
    "# logger.info(f'{len(seq_dataset)} records')\n",
    "\n",
    "# # double check: all xsum\n",
    "# for _record in seq_dataset:\n",
    "#     assert _record['dataset_name'] == 'xsum'\n",
    "# # end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a165540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 250709 10:05:20 2043887377:11] 2400 records\n"
     ]
    }
   ],
   "source": [
    "# case CNN constraints dataset\n",
    "import json\n",
    "\n",
    "PATH_CONSTRAINS_CNN = Path(\"/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/datasets/constraints_fact_v1.0/cnn_dailymail/collect.json\")\n",
    "assert PATH_CONSTRAINS_CNN.exists()\n",
    "\n",
    "with PATH_CONSTRAINS_CNN.open() as f:\n",
    "    seq_dataset = [json.loads(_line) for _line in f.readlines()]\n",
    "# end with\n",
    "\n",
    "logger.info(f'{len(seq_dataset)} records')\n",
    "\n",
    "# double check: all xsum\n",
    "for _record in seq_dataset:\n",
    "    assert _record['dataset_name'] == 'cnn_dailymail'\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0398b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_and_summary(record_obj: ty.Dict) -> ty.Tuple[str, str]:\n",
    "    # return record_obj['document_original'], record_obj['summary_raw']\n",
    "    return record_obj['document_full'], record_obj['summary_raw']\n",
    "# end def\n",
    "\n",
    "target_document_index = [1, 10, 100, 200]\n",
    "\n",
    "import pprint\n",
    "\n",
    "seq_stack = []\n",
    "\n",
    "dict_commnad2ep = dict(\n",
    "    lambda4 = 'log_exp(2,4.804488)',  # lambda4\n",
    "    lambda2 = 'log_exp(2,2.402244)',  # lambda2\n",
    "    lambda1 = 'log_exp(2,1.201122)',  # lambda1\n",
    "    none = 'none()',\n",
    "    linear = 'linear()',\n",
    ")\n",
    "dict_commnad2ep['1/lambda2'] = 'log_exp(2,0.416277447)'  # 1/lambda2, log_exp(2, 1 / (1.20112 * 2))\n",
    "dict_commnad2ep['1/lambda1'] = 'log_exp(2,0.832556281)'  # 1/lambda1, log_exp(2, 1 / 1.20112)\n",
    "\n",
    "\n",
    "# for _idx in target_document_index:\n",
    "#     _record = seq_dataset[_idx]\n",
    "\n",
    "#     _document_id: str = _record['document_id']\n",
    "#     command_abstractiveness_constraint: str = _record['abstractiveness_constraint']\n",
    "\n",
    "#     _document_original, _summary_raw = get_source_and_summary(_record)\n",
    "#     extractive_penalty_fct = dict_commnad2ep[command_abstractiveness_constraint]\n",
    "\n",
    "#     seq_summary = bart_sample(\n",
    "#         bart=bart_model,\n",
    "#         batch=[_document_original],\n",
    "#         extractive_penalty_fct=extractive_penalty_fct\n",
    "#     )\n",
    "\n",
    "#     _res_obj = dict(\n",
    "#         document_id=_document_id,\n",
    "#         document_original=_document_original,\n",
    "#         summary_raw=_summary_raw,\n",
    "#         summary_gen=seq_summary[0],\n",
    "#         extractive_penalty_fct=extractive_penalty_fct,\n",
    "#         command_abstractiveness_constraint=command_abstractiveness_constraint\n",
    "#     )\n",
    "#     seq_stack.append(_res_obj)\n",
    "# # end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 250709 10:24:05 2610531437:10] Loading model /workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.cnn/model.pt\n",
      "[I 250709 10:24:25 2610531437:17] Loading done.\n",
      "[I 250709 10:24:29 2610531437:10] Loading model /workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.cnn/model.pt\n",
      "[I 250709 10:24:47 2610531437:17] Loading done.\n"
     ]
    }
   ],
   "source": [
    "def get_extractive_penalty_fct(penalty_command: str) -> str:\n",
    "    dict_commnad2ep = dict(\n",
    "        lambda4 = 'log_exp(2,4.804488)',  # lambda4\n",
    "        lambda2 = 'log_exp(2,2.402244)',  # lambda2\n",
    "        lambda1 = 'log_exp(2,1.201122)',  # lambda1\n",
    "        none = 'none()',\n",
    "        linear = 'linear()',\n",
    "    )\n",
    "    dict_commnad2ep['1/lambda2'] = 'log_exp(2,0.416277447)'  # 1/lambda2, log_exp(2, 1 / (1.20112 * 2))\n",
    "    dict_commnad2ep['1/lambda1'] = 'log_exp(2,0.832556281)'  # 1/lambda1, log_exp(2, 1 / 1.20112)\n",
    "\n",
    "    assert penalty_command in dict_commnad2ep\n",
    "\n",
    "    return dict_commnad2ep[penalty_command]\n",
    "\n",
    "\n",
    "def init_model(testresource: Path) -> BARTHubInterface:\n",
    "    path_model = testresource / 'models/bart.large.cnn'\n",
    "    assert path_model.exists(), f'no model found at {path_model}'\n",
    "\n",
    "    bart_model = load_model(path_model, path_model / 'model.pt')\n",
    "\n",
    "    return bart_model\n",
    "\n",
    "def test_stochastic_sampling_sample_same(testresource: Path):\n",
    "    bart_model = init_model(testresource)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device_obj = torch.device('cuda:0')\n",
    "    else:\n",
    "        device_obj = torch.device('cpu')\n",
    "    # end if\n",
    "\n",
    "    bart_model = bart_model.to(device_obj)    \n",
    "\n",
    "    document_input = 'He\\'s a blue chip college basketball recruit. She\\'s a high school freshman with Down syndrome. At first glance Trey Moses and Ellie Meredith couldn\\'t be more different. But all that changed Thursday when Trey asked Ellie to be his prom date. Trey -- a star on Eastern High School\\'s basketball team in Louisville, Kentucky, who\\'s headed to play college ball next year at Ball State -- was originally going to take his girlfriend to Eastern\\'s prom. So why is he taking Ellie instead? \"She\\'s great... she listens and she\\'s easy to talk to\" he said. Trey made the prom-posal (yes, that\\'s what they are calling invites to prom these days) in the gym during Ellie\\'s P.E. class. Trina Helson, a teacher at Eastern, alerted the school\\'s newspaper staff to the prom-posal and posted photos of Trey and Ellie on Twitter that have gone viral. She wasn\\'t surpristed by Trey\\'s actions. \"That\\'s the kind of person Trey is,\" she said. To help make sure she said yes, Trey entered the gym armed with flowers and a poster that read \"Let\\'s Party Like it\\'s 1989,\" a reference to the latest album by Taylor Swift, Ellie\\'s favorite singer. Trey also got the OK from Ellie\\'s parents the night before via text. They were thrilled. \"You just feel numb to those moments raising a special needs child,\"  said Darla Meredith, Ellie\\'s mom. \"You first feel the need to protect and then to overprotect.\" Darla Meredith said Ellie has struggled with friendships since elementary school, but a special program at Eastern called Best Buddies had made things easier for her. She said Best Buddies cultivates friendships between students with and without developmental disabilities and prevents students like Ellie from feeling isolated and left out of social functions. \"I guess around middle school is when kids started to care about what others thought,\" she said, but \"this school, this year has been a relief.\" Trey\\'s future coach at Ball State, James Whitford, said he felt great about the prom-posal, noting that Trey, whom he\\'s known for a long time, often works with other kids . Trey\\'s mother, Shelly Moses, was also proud of her son. \"It\\'s exciting to bring awareness to a good cause,\" she said. \"Trey has worked pretty hard, and he\\'s a good son.\" Both Trey and Ellie have a lot of planning to do. Trey is looking to take up special education as a college major, in addition to playing basketball in the fall. As for Ellie, she can\\'t stop thinking about prom. \"Ellie can\\'t wait to go dress shopping\" her mother said. \"Because I\\'ve only told about a million people!\" Ellie interjected.'\n",
    "    extractive_penalty_fct = get_extractive_penalty_fct('none')\n",
    "\n",
    "    random_seed = 42\n",
    "    seq_summary_stochastic_1st = bart_sample_stochastic(\n",
    "        bart=bart_model,\n",
    "        temperature=1.0,\n",
    "        batch=[document_input],\n",
    "        random_seed_value=random_seed,\n",
    "        extractive_penalty_fct=extractive_penalty_fct)\n",
    "    \n",
    "    seq_summary_stochastic_2nd = bart_sample_stochastic(\n",
    "        bart=bart_model,\n",
    "        temperature=1.0,\n",
    "        batch=[document_input],\n",
    "        random_seed_value=random_seed,\n",
    "        extractive_penalty_fct=extractive_penalty_fct)\n",
    "    \n",
    "    assert seq_summary_stochastic_1st[0] == seq_summary_stochastic_2nd[0]\n",
    "\n",
    "\n",
    "def test_stochastic_sampling_sample_variant(testresource: Path):\n",
    "    bart_model = init_model(testresource)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device_obj = torch.device('cuda:0')\n",
    "    else:\n",
    "        device_obj = torch.device('cpu')\n",
    "    # end if\n",
    "\n",
    "    bart_model = bart_model.to(device_obj)    \n",
    "\n",
    "    document_input = 'He\\'s a blue chip college basketball recruit. She\\'s a high school freshman with Down syndrome. At first glance Trey Moses and Ellie Meredith couldn\\'t be more different. But all that changed Thursday when Trey asked Ellie to be his prom date. Trey -- a star on Eastern High School\\'s basketball team in Louisville, Kentucky, who\\'s headed to play college ball next year at Ball State -- was originally going to take his girlfriend to Eastern\\'s prom. So why is he taking Ellie instead? \"She\\'s great... she listens and she\\'s easy to talk to\" he said. Trey made the prom-posal (yes, that\\'s what they are calling invites to prom these days) in the gym during Ellie\\'s P.E. class. Trina Helson, a teacher at Eastern, alerted the school\\'s newspaper staff to the prom-posal and posted photos of Trey and Ellie on Twitter that have gone viral. She wasn\\'t surpristed by Trey\\'s actions. \"That\\'s the kind of person Trey is,\" she said. To help make sure she said yes, Trey entered the gym armed with flowers and a poster that read \"Let\\'s Party Like it\\'s 1989,\" a reference to the latest album by Taylor Swift, Ellie\\'s favorite singer. Trey also got the OK from Ellie\\'s parents the night before via text. They were thrilled. \"You just feel numb to those moments raising a special needs child,\"  said Darla Meredith, Ellie\\'s mom. \"You first feel the need to protect and then to overprotect.\" Darla Meredith said Ellie has struggled with friendships since elementary school, but a special program at Eastern called Best Buddies had made things easier for her. She said Best Buddies cultivates friendships between students with and without developmental disabilities and prevents students like Ellie from feeling isolated and left out of social functions. \"I guess around middle school is when kids started to care about what others thought,\" she said, but \"this school, this year has been a relief.\" Trey\\'s future coach at Ball State, James Whitford, said he felt great about the prom-posal, noting that Trey, whom he\\'s known for a long time, often works with other kids . Trey\\'s mother, Shelly Moses, was also proud of her son. \"It\\'s exciting to bring awareness to a good cause,\" she said. \"Trey has worked pretty hard, and he\\'s a good son.\" Both Trey and Ellie have a lot of planning to do. Trey is looking to take up special education as a college major, in addition to playing basketball in the fall. As for Ellie, she can\\'t stop thinking about prom. \"Ellie can\\'t wait to go dress shopping\" her mother said. \"Because I\\'ve only told about a million people!\" Ellie interjected.'\n",
    "    extractive_penalty_fct = get_extractive_penalty_fct('none')\n",
    "\n",
    "    random_seed = 42\n",
    "    seq_summary_stochastic_1st = bart_sample_stochastic(\n",
    "        bart=bart_model,\n",
    "        temperature=1.0,\n",
    "        batch=[document_input],\n",
    "        random_seed_value=random_seed,\n",
    "        extractive_penalty_fct=extractive_penalty_fct)\n",
    "    \n",
    "    random_seed = 1\n",
    "    seq_summary_stochastic_2nd = bart_sample_stochastic(\n",
    "        bart=bart_model,\n",
    "        temperature=1.0,\n",
    "        batch=[document_input],\n",
    "        random_seed_value=random_seed,\n",
    "        extractive_penalty_fct=extractive_penalty_fct)\n",
    "    \n",
    "    assert seq_summary_stochastic_1st[0] != seq_summary_stochastic_2nd[0]\n",
    "\n",
    "# --------------------------\n",
    "\n",
    "path_resources = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources')\n",
    "\n",
    "test_stochastic_sampling_sample_same(path_resources)\n",
    "test_stochastic_sampling_sample_variant(path_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmitsuzawa/.local/miniconda3/envs/p39-Dreyer-2023/lib/python3.9/site-packages/torch/random.py:99: UserWarning: CUDA reports that you have 2 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of GPUs.  If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using.  For example, if you are using CPU only, set CUDA_VISIBLE_DEVICES= or devices=[]; if you are using GPU 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'document_id': '10',\n",
       " 'document_original': 'He\\'s a blue chip college basketball recruit. She\\'s a high school freshman with Down syndrome. At first glance Trey Moses and Ellie Meredith couldn\\'t be more different. But all that changed Thursday when Trey asked Ellie to be his prom date. Trey -- a star on Eastern High School\\'s basketball team in Louisville, Kentucky, who\\'s headed to play college ball next year at Ball State -- was originally going to take his girlfriend to Eastern\\'s prom. So why is he taking Ellie instead? \"She\\'s great... she listens and she\\'s easy to talk to\" he said. Trey made the prom-posal (yes, that\\'s what they are calling invites to prom these days) in the gym during Ellie\\'s P.E. class. Trina Helson, a teacher at Eastern, alerted the school\\'s newspaper staff to the prom-posal and posted photos of Trey and Ellie on Twitter that have gone viral. She wasn\\'t surpristed by Trey\\'s actions. \"That\\'s the kind of person Trey is,\" she said. To help make sure she said yes, Trey entered the gym armed with flowers and a poster that read \"Let\\'s Party Like it\\'s 1989,\" a reference to the latest album by Taylor Swift, Ellie\\'s favorite singer. Trey also got the OK from Ellie\\'s parents the night before via text. They were thrilled. \"You just feel numb to those moments raising a special needs child,\"  said Darla Meredith, Ellie\\'s mom. \"You first feel the need to protect and then to overprotect.\" Darla Meredith said Ellie has struggled with friendships since elementary school, but a special program at Eastern called Best Buddies had made things easier for her. She said Best Buddies cultivates friendships between students with and without developmental disabilities and prevents students like Ellie from feeling isolated and left out of social functions. \"I guess around middle school is when kids started to care about what others thought,\" she said, but \"this school, this year has been a relief.\" Trey\\'s future coach at Ball State, James Whitford, said he felt great about the prom-posal, noting that Trey, whom he\\'s known for a long time, often works with other kids . Trey\\'s mother, Shelly Moses, was also proud of her son. \"It\\'s exciting to bring awareness to a good cause,\" she said. \"Trey has worked pretty hard, and he\\'s a good son.\" Both Trey and Ellie have a lot of planning to do. Trey is looking to take up special education as a college major, in addition to playing basketball in the fall. As for Ellie, she can\\'t stop thinking about prom. \"Ellie can\\'t wait to go dress shopping\" her mother said. \"Because I\\'ve only told about a million people!\" Ellie interjected.',\n",
       " 'summary_raw': 'Trey Moses asked Ellie Meredith to be his prom date. Ellie has Down syndrome and Trey is a basketball star at Eastern High School in Louisville, Kentucky. Photos of the couple have gone viral on Twitter. Ellie\\'s mom: \"You just feel numb to those moments\"',\n",
       " 'summary_gen': 'Trey Moses asked Ellie Meredith to be his prom date. Trey is a star basketball player at Eastern High School in Louisville, Kentucky. Ellie is a freshman with Down syndrome. Photos of the couple have gone viral on social media and in the news. \"That\\'s the kind of person Trey is,\" Ellie\\'s mom says.',\n",
       " 'summary_gen_stochastic': 'Trey Moses is a star basketball player at Eastern High Party in Kentucky. Ellis Meredith, who has Down syndrome, entered the gym with flowers for the prom director. A teacher at the school witnessed the proposal and alerted the NICU. The photos of Trey and Ellie at the ceremony have gone viraldaughter Osiris, a special ed teacher at Eastern, tweeted about it.',\n",
       " 'extractive_penalty_fct': 'none()',\n",
       " 'command_abstractiveness_constraint': 'none'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_idx = 10\n",
    "\n",
    "_record = seq_dataset[_idx]\n",
    "\n",
    "_document_id: str = _record['document_id']\n",
    "command_abstractiveness_constraint: str = _record['abstractiveness_constraint']\n",
    "\n",
    "_document_original, _summary_raw = get_source_and_summary(_record)\n",
    "extractive_penalty_fct = dict_commnad2ep[command_abstractiveness_constraint]\n",
    "\n",
    "seq_summary_beam = bart_sample(\n",
    "    bart=bart_model,\n",
    "    batch=[_document_original],\n",
    "    extractive_penalty_fct=extractive_penalty_fct\n",
    ")\n",
    "\n",
    "\n",
    "seq_summary_stochastic = bart_sample_stochastic(\n",
    "    bart=bart_model,\n",
    "    temperature=1.0,\n",
    "    batch=[_document_original],\n",
    "    extractive_penalty_fct=extractive_penalty_fct,\n",
    "    random_seed_value=42\n",
    ")\n",
    "\n",
    "\n",
    "_res_obj = dict(\n",
    "    document_id=_document_id,\n",
    "    document_original=_document_original,\n",
    "    summary_raw=_summary_raw,\n",
    "    summary_gen=seq_summary_beam[0],\n",
    "    summary_gen_stochastic=seq_summary_stochastic[0],\n",
    "    extractive_penalty_fct=extractive_penalty_fct,\n",
    "    command_abstractiveness_constraint=command_abstractiveness_constraint\n",
    ")\n",
    "\n",
    "\n",
    "_res_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3699f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Trey Moses is a star basketball player at Eastern High Party in Kentucky. Ellis Meredith, who has Down syndrome, entered the gym with flowers for the prom director. A teacher at the school witnessed the proposal and alerted the NICU. The photos of Trey and Ellie at the ceremony have gone viraldaughter Osiris, a special ed teacher at Eastern, tweeted about it.'],\n",
       " [\" Excellence required at high school; basketball player decides to take Down syndrome girl to prom. Teacher posts photos of couple on�� on Lollapalooza;itement goes viral. Trey's now taking Ellie, an admirer of Down syndrome kids, as his date. Ellie's mother says empowering special education students janitionally helps eased Ellie's social struggles.\"])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary_stochastic_42 = bart_sample_stochastic(\n",
    "#     bart=bart_model,\n",
    "#     temperature=1.0,\n",
    "#     batch=[_document_original],\n",
    "#     extractive_penalty_fct=extractive_penalty_fct,\n",
    "#     random_seed_value=42\n",
    "# )\n",
    "\n",
    "# summary_stochastic_24 = bart_sample_stochastic(\n",
    "#     bart=bart_model,\n",
    "#     temperature=1.0,\n",
    "#     batch=[_document_original],\n",
    "#     extractive_penalty_fct=extractive_penalty_fct,\n",
    "#     random_seed_value=24\n",
    "# )\n",
    "\n",
    "# summary_stochastic_42, summary_stochastic_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff75475",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_extractive_penalty_fct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m document_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHe\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms a blue chip college basketball recruit. She\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms a high school freshman with Down syndrome. At first glance Trey Moses and Ellie Meredith couldn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt be more different. But all that changed Thursday when Trey asked Ellie to be his prom date. Trey -- a star on Eastern High School\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms basketball team in Louisville, Kentucky, who\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms headed to play college ball next year at Ball State -- was originally going to take his girlfriend to Eastern\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms prom. So why is he taking Ellie instead? \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShe\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms great... she listens and she\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms easy to talk to\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m he said. Trey made the prom-posal (yes, that\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms what they are calling invites to prom these days) in the gym during Ellie\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms P.E. class. Trina Helson, a teacher at Eastern, alerted the school\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms newspaper staff to the prom-posal and posted photos of Trey and Ellie on Twitter that have gone viral. She wasn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt surpristed by Trey\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms actions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThat\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms the kind of person Trey is,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m she said. To help make sure she said yes, Trey entered the gym armed with flowers and a poster that read \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLet\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms Party Like it\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms 1989,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a reference to the latest album by Taylor Swift, Ellie\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms favorite singer. Trey also got the OK from Ellie\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms parents the night before via text. They were thrilled. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou just feel numb to those moments raising a special needs child,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  said Darla Meredith, Ellie\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms mom. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou first feel the need to protect and then to overprotect.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Darla Meredith said Ellie has struggled with friendships since elementary school, but a special program at Eastern called Best Buddies had made things easier for her. She said Best Buddies cultivates friendships between students with and without developmental disabilities and prevents students like Ellie from feeling isolated and left out of social functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI guess around middle school is when kids started to care about what others thought,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m she said, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis school, this year has been a relief.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Trey\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms future coach at Ball State, James Whitford, said he felt great about the prom-posal, noting that Trey, whom he\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms known for a long time, often works with other kids . Trey\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms mother, Shelly Moses, was also proud of her son. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms exciting to bring awareness to a good cause,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m she said. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrey has worked pretty hard, and he\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms a good son.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Both Trey and Ellie have a lot of planning to do. Trey is looking to take up special education as a college major, in addition to playing basketball in the fall. As for Ellie, she can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt stop thinking about prom. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEllie can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt wait to go dress shopping\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m her mother said. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBecause I\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mve only told about a million people!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Ellie interjected.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m extractive_penalty_fct \u001b[38;5;241m=\u001b[39m \u001b[43mget_extractive_penalty_fct\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# tensor_source_tokens = bart_model.encode([document_input])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# tensor_source_tokens = torch.tensor([tensor_source_tokens]).to(bart_model.device)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m beam: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_extractive_penalty_fct' is not defined"
     ]
    }
   ],
   "source": [
    "# document_input = 'He\\'s a blue chip college basketball recruit. She\\'s a high school freshman with Down syndrome. At first glance Trey Moses and Ellie Meredith couldn\\'t be more different. But all that changed Thursday when Trey asked Ellie to be his prom date. Trey -- a star on Eastern High School\\'s basketball team in Louisville, Kentucky, who\\'s headed to play college ball next year at Ball State -- was originally going to take his girlfriend to Eastern\\'s prom. So why is he taking Ellie instead? \"She\\'s great... she listens and she\\'s easy to talk to\" he said. Trey made the prom-posal (yes, that\\'s what they are calling invites to prom these days) in the gym during Ellie\\'s P.E. class. Trina Helson, a teacher at Eastern, alerted the school\\'s newspaper staff to the prom-posal and posted photos of Trey and Ellie on Twitter that have gone viral. She wasn\\'t surpristed by Trey\\'s actions. \"That\\'s the kind of person Trey is,\" she said. To help make sure she said yes, Trey entered the gym armed with flowers and a poster that read \"Let\\'s Party Like it\\'s 1989,\" a reference to the latest album by Taylor Swift, Ellie\\'s favorite singer. Trey also got the OK from Ellie\\'s parents the night before via text. They were thrilled. \"You just feel numb to those moments raising a special needs child,\"  said Darla Meredith, Ellie\\'s mom. \"You first feel the need to protect and then to overprotect.\" Darla Meredith said Ellie has struggled with friendships since elementary school, but a special program at Eastern called Best Buddies had made things easier for her. She said Best Buddies cultivates friendships between students with and without developmental disabilities and prevents students like Ellie from feeling isolated and left out of social functions. \"I guess around middle school is when kids started to care about what others thought,\" she said, but \"this school, this year has been a relief.\" Trey\\'s future coach at Ball State, James Whitford, said he felt great about the prom-posal, noting that Trey, whom he\\'s known for a long time, often works with other kids . Trey\\'s mother, Shelly Moses, was also proud of her son. \"It\\'s exciting to bring awareness to a good cause,\" she said. \"Trey has worked pretty hard, and he\\'s a good son.\" Both Trey and Ellie have a lot of planning to do. Trey is looking to take up special education as a college major, in addition to playing basketball in the fall. As for Ellie, she can\\'t stop thinking about prom. \"Ellie can\\'t wait to go dress shopping\" her mother said. \"Because I\\'ve only told about a million people!\" Ellie interjected.'\n",
    "# extractive_penalty_fct = get_extractive_penalty_fct('none')\n",
    "\n",
    "# # tensor_source_tokens = bart_model.encode([document_input])\n",
    "# # tensor_source_tokens = torch.tensor([tensor_source_tokens]).to(bart_model.device)\n",
    "\n",
    "# beam: int = 4\n",
    "# lenpen: float = 2.0  # length penalty\n",
    "# min_len: int = 55\n",
    "# max_len_a: int = 0\n",
    "# max_len_b: int = 140\n",
    "# no_repeat_ngram_size: int = 3\n",
    "\n",
    "# random_seed_value = 42\n",
    "\n",
    "# dict_parameters = dict(\n",
    "#     beam=beam,\n",
    "#     lenpen=lenpen,\n",
    "#     sampling=True,\n",
    "#     temperature=1.0,\n",
    "#     min_len=min_len, \n",
    "#     max_len_a=max_len_a, \n",
    "#     max_len_b=max_len_b,\n",
    "#     no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "#     extractive_penalty_fct=extractive_penalty_fct)\n",
    "\n",
    "# with (torch.random.fork_rng(), torch.no_grad()):\n",
    "#     torch.manual_seed(random_seed_value)\n",
    "#     torch.cuda.manual_seed_all(random_seed_value)  # if you are using multi-GPU.        \n",
    "\n",
    "#     res = bart_model.generate(tensor_source_tokens)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39-Dreyer-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
