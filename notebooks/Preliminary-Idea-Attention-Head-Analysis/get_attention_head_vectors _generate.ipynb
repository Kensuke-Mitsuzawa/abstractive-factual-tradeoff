{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ea2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "import typing as ty\n",
    "\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "from torch.multiprocessing import Pool, set_start_method\n",
    "set_start_method('spawn', force=True)\n",
    "from functools import partial\n",
    "import more_itertools as mit\n",
    "\n",
    "import torch\n",
    "import fairseq\n",
    "from fairseq.models.bart import BARTHubInterface\n",
    "from fairseq.models.bart import BARTModel\n",
    "\n",
    "import nvgpu\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a3577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logzero\n",
    "\n",
    "from datetime import datetime\n",
    "_datetime_exec = datetime.now()\n",
    "\n",
    "logzero.logfile(f\"logs/{_datetime_exec.isoformat()}.log\")\n",
    "\n",
    "logger = logzero.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2406b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(task: Path, model_path: Path) -> BARTHubInterface:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        task: a path to the directory of the model.\n",
    "        model_path: a path to 'model.pt' file.\n",
    "    \"\"\"\n",
    "    assert task.exists()\n",
    "    assert model_path.exists()\n",
    "\n",
    "    logger.info(f\"Loading model {model_path}\")\n",
    "    model_dirname, model_fname = os.path.split(model_path.as_posix())\n",
    "    bart = BARTModel.from_pretrained(\n",
    "        model_dirname,\n",
    "        checkpoint_file=model_fname,\n",
    "        data_name_or_path=task.as_posix()\n",
    "    )\n",
    "    logger.info(f\"Loading done.\")\n",
    "    return bart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d649815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to input\n",
    "PATH_TEXT_FILE_INPUT = Path(\"/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/xsum/test_source.txt\")\n",
    "assert PATH_TEXT_FILE_INPUT.exists()\n",
    "\n",
    "seq_text_input = PATH_TEXT_FILE_INPUT.open().readlines()\n",
    "assert len(seq_text_input) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdc80746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 250718 07:54:11 2610531437:10] Loading model /workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.xsum/model.pt\n",
      "[I 250718 07:54:26 2610531437:17] Loading done.\n"
     ]
    }
   ],
   "source": [
    "# with xsum model\n",
    "PATH_MODEL_FILE = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.xsum')\n",
    "# with cnn model\n",
    "# PATH_MODEL_FILE = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.cnn')\n",
    "\n",
    "bart_model = load_model(PATH_MODEL_FILE, PATH_MODEL_FILE / 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a8cb99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_obj = torch.device('cuda:0')\n",
    "else:\n",
    "    device_obj = torch.device('cpu')\n",
    "# end if\n",
    "\n",
    "bart_model = bart_model.to(device_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930cdf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 250718 06:50:21 3352606314:11] 3000 records\n"
     ]
    }
   ],
   "source": [
    "# case Xsum constraints dataset\n",
    "import json\n",
    "\n",
    "PATH_CONSTRAINS_XSUM = Path(\"/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/datasets/constraints_fact_v1.0/xsum/collect.json\")\n",
    "assert PATH_CONSTRAINS_XSUM.exists()\n",
    "\n",
    "with PATH_CONSTRAINS_XSUM.open() as f:\n",
    "    seq_dataset = [json.loads(_line) for _line in f.readlines()]\n",
    "# end with\n",
    "\n",
    "logger.info(f'{len(seq_dataset)} records')\n",
    "\n",
    "# double check: all xsum\n",
    "for _record in seq_dataset:\n",
    "    assert _record['dataset_name'] == 'xsum'\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4362f321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmitsuzawa/.local/miniconda3/envs/p39-Dreyer-2023-2nd/lib/python3.9/site-packages/torch/random.py:159: UserWarning: CUDA reports that you have 2 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of CUDAs. If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using. For example, if you are using CPU only, set device.upper()_VISIBLE_DEVICES= or devices=[]; if you are using device 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "def get_extractive_penalty_fct(penalty_command: str) -> str:\n",
    "    dict_commnad2ep = dict(\n",
    "        lambda4 = 'log_exp(2,4.804488)',  # lambda4\n",
    "        lambda2 = 'log_exp(2,2.402244)',  # lambda2\n",
    "        lambda1 = 'log_exp(2,1.201122)',  # lambda1\n",
    "        none = 'none()',\n",
    "        linear = 'linear()',\n",
    "    )\n",
    "    dict_commnad2ep['1/lambda2'] = 'log_exp(2,0.416277447)'  # 1/lambda2, log_exp(2, 1 / (1.20112 * 2))\n",
    "    dict_commnad2ep['1/lambda1'] = 'log_exp(2,0.832556281)'  # 1/lambda1, log_exp(2, 1 / 1.20112)\n",
    "\n",
    "    assert penalty_command in dict_commnad2ep\n",
    "\n",
    "    return dict_commnad2ep[penalty_command]\n",
    "\n",
    "\n",
    "def bart_sample(bart_model: BARTHubInterface,\n",
    "                batch: ty.List[str],\n",
    "                extractive_penalty_fct: str,\n",
    "                beam: int = 4,\n",
    "                lenpen: float = 2.0,  # length penalty\n",
    "                min_len: int = 55,\n",
    "                max_len_a: int = 0,\n",
    "                max_len_b: int = 140,\n",
    "                no_repeat_ngram_size: int = 3):\n",
    "    \n",
    "    # lenpen: float = 2.0  # length penalty\n",
    "    # min_len: int = 55\n",
    "    # max_len_a: int = 0\n",
    "    # max_len_b: int = 140\n",
    "    # no_repeat_ngram_size: int = 3\n",
    "    extractive_penalty_fct = get_extractive_penalty_fct('none')\n",
    "\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)  # if you are using multi-GPU.\n",
    "\n",
    "        # tau = 0.1\n",
    "        dict_parameters = dict(\n",
    "            beam=beam,\n",
    "            lenpen=lenpen,\n",
    "            sampling=False,\n",
    "            min_len=min_len, \n",
    "            max_len_a=max_len_a, \n",
    "            max_len_b=max_len_b,\n",
    "            # temperature=0.1,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            extractive_penalty_fct=extractive_penalty_fct)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tensor_input_ids = [bart_model.encode(text) for text in  batch]\n",
    "            tensor_stack = torch.stack(tensor_input_ids).to(bart_model.device)\n",
    "            generated_ids = bart_model.generate(tensor_stack, **dict_parameters)\n",
    "            text_summary_generate_method: str = bart_model.decode(generated_ids[0]['tokens'])\n",
    "    \n",
    "# end def\n",
    "\n",
    "\n",
    "\n",
    "def get_source_and_summary(record_obj: ty.Dict) -> ty.Tuple[str, str]:\n",
    "    # return record_obj['document_original'], record_obj['summary_raw']\n",
    "    return record_obj['document_full'], record_obj['summary_raw']\n",
    "# end def\n",
    "\n",
    "target_document_index = [1, 10, 100, 200]\n",
    "\n",
    "import pprint\n",
    "\n",
    "seq_stack = []\n",
    "\n",
    "dict_commnad2ep = dict(\n",
    "    lambda4 = 'log_exp(2,4.804488)',  # lambda4\n",
    "    lambda2 = 'log_exp(2,2.402244)',  # lambda2\n",
    "    lambda1 = 'log_exp(2,1.201122)',  # lambda1\n",
    "    none = 'none()',\n",
    "    linear = 'linear()',\n",
    ")\n",
    "dict_commnad2ep['1/lambda2'] = 'log_exp(2,0.416277447)'  # 1/lambda2, log_exp(2, 1 / (1.20112 * 2))\n",
    "dict_commnad2ep['1/lambda1'] = 'log_exp(2,0.832556281)'  # 1/lambda1, log_exp(2, 1 / 1.20112)\n",
    "\n",
    "\n",
    "for _idx in target_document_index:\n",
    "    _record = seq_dataset[_idx]\n",
    "\n",
    "    _document_id: str = _record['document_id']\n",
    "    command_abstractiveness_constraint: str = _record['abstractiveness_constraint']\n",
    "\n",
    "    _document_original, _summary_raw = get_source_and_summary(_record)\n",
    "    extractive_penalty_fct = dict_commnad2ep[command_abstractiveness_constraint]\n",
    "\n",
    "    seq_summary = bart_sample(\n",
    "        bart_model=bart_model,\n",
    "        batch=[_document_original],\n",
    "        extractive_penalty_fct=extractive_penalty_fct\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with xsum model\n",
    "PATH_MODEL_FILE = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.xsum')\n",
    "# with cnn model\n",
    "# PATH_MODEL_FILE = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.cnn')\n",
    "\n",
    "bart_model = load_model(PATH_MODEL_FILE, PATH_MODEL_FILE / 'model.pt')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_obj = torch.device('cuda:0')\n",
    "else:\n",
    "    device_obj = torch.device('cpu')\n",
    "# end if\n",
    "\n",
    "bart_model = bart_model.to(device_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55ec9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 0\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 1\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 2\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 3\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 4\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 5\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 6\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 7\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 8\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 9\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 10\n",
      "[D 250718 07:54:36 1771820259:31] Setting attention_hook at a layer at 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A novel method for detecting hallucinations in large language models has been proposed.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.sequence_generator import SequenceGenerator\n",
    "from functools import wraps\n",
    "\n",
    "# ------------------------------------\n",
    "# setting hook to extract the attention heads\n",
    "attn_data = {}\n",
    "\n",
    "def get_attention_hook(layer_idx, attn_type):\n",
    "    def hook(module, input, output):\n",
    "        # if output is not None and isinstance(output, tuple) and output[1] is not None:\n",
    "        #     # print(layer_idx, attn_type)\n",
    "        #     attn_data.setdefault((layer_idx, attn_type), (output[1].detach().cpu()))\n",
    "        \n",
    "        # Check output format from MultiheadAttention.forward\n",
    "        # It can return (attn_output, attn_weights) if `need_weights=True`\n",
    "        if isinstance(output, tuple):\n",
    "            _, attn_weights = output\n",
    "            if attn_weights is not None:\n",
    "                attn_data[(layer_idx, attn_type)] = attn_weights.detach().cpu()\n",
    "            # end if\n",
    "        # end if\n",
    "    return hook\n",
    "# end def\n",
    "\n",
    "# Register hooks for all decoder layers\n",
    "self_attn_hooks = []\n",
    "cross_attn_hooks = []\n",
    "for i, layer in enumerate(bart_model.model.decoder.layers):\n",
    "    layer.self_attn.register_forward_hook(get_attention_hook(i, 'self'))\n",
    "    layer.encoder_attn.register_forward_hook(get_attention_hook(i, 'cross'))\n",
    "    logger.debug(f'Setting attention_hook at a layer at {i}')\n",
    "# end for\n",
    "\n",
    "# ------------------------------------\n",
    "# setting the internal parameters True for extracting the attention heads.\n",
    "# for layer in encoder_decoder_interface.model.decoder.layers:\n",
    "#     def wrap_forward(original_forward):\n",
    "#         def new_forward(*args, **kwargs):\n",
    "#             kwargs['need_attn'] = True\n",
    "#             kwargs['need_head_weights'] = True\n",
    "#             return original_forward(*args, **kwargs)\n",
    "#         return new_forward\n",
    "#     layer.forward = wrap_forward(layer.forward)\n",
    "# setting the flag to the self-attention layers.\n",
    "# overwriting the foward method of `MultiheadAttention`. The default value is False and attn_weights are averaged automatically.\n",
    "# https://github.com/facebookresearch/fairseq/blob/d13e14a800bb588e5a77fb4e551f554ff9b24a72/fairseq/modules/multihead_attention.py#L469\n",
    "for i, layer in enumerate(bart_model.model.decoder.layers):\n",
    "    orig_self_attn_forward = layer.self_attn.forward\n",
    "    orig_encoder_attn_forward = layer.encoder_attn.forward        \n",
    "\n",
    "    # @wraps(orig_self_attn_forward)\n",
    "    # def wrapped_self_attn_forward(*args, **kwargs):\n",
    "    #     kwargs['need_weights'] = True\n",
    "    #     kwargs['need_head_weights'] = True\n",
    "    #     return orig_self_attn_forward(*args, **kwargs)\n",
    "\n",
    "    # @wraps(orig_encoder_attn_forward)\n",
    "    # def wrapped_encoder_attn_forward(*args, **kwargs):\n",
    "    #     kwargs['need_weights'] = True\n",
    "    #     kwargs['need_head_weights'] = True\n",
    "    #     return orig_encoder_attn_forward(*args, **kwargs)\n",
    "\n",
    "    # layer.self_attn.forward = wrapped_self_attn_forward\n",
    "    # layer.encoder_attn.forward = wrapped_encoder_attn_forward        \n",
    "# ------------------------------------\n",
    "\n",
    "\n",
    "generator = SequenceGenerator(\n",
    "    models=[bart_model.model],\n",
    "    tgt_dict=bart_model.task.target_dictionary,\n",
    "    beam_size=4,\n",
    "    len_penalty=1.0,\n",
    "    max_len_b=200,\n",
    "    min_len=1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    extractive_penalty_fct=\"none()\"\n",
    ")\n",
    "\n",
    "\n",
    "source_text = \"We present a novel approach for detecting hallucinations in large language models (LLMs) by analyzing the probabilistic divergence between prompt and response hiddenstate distributions. Counterintuitively, we find that hallucinated responses exhibit smaller deviations from their prompts compared to grounded responses, suggesting that hallucinations often arise from superficial rephrasing rather than substantive reasoning. Leveraging this insight, we propose a model-intrinsic detection method1 that uses distributional distances as principled hallucination scores, eliminating the need for external knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable kernels that automatically adapt to capture nuanced geometric differences between distributions. Our approach outperforms existing baselines, demonstrating state-of-the-art performance on several benchmarks. The method remains competitive even without kernel training, offering a robust, scalable solution for hallucination detection.\"\n",
    "source_ids = bart_model.encode(source_text).unsqueeze(0).to(device_obj)\n",
    "\n",
    "encoder_out = bart_model.model.encoder.forward(source_ids, src_lengths=None)\n",
    "\n",
    "output = generator.generate(\n",
    "    models=[bart_model.model],\n",
    "    sample={'net_input': {'src_tokens': source_ids, 'src_lengths': None}},\n",
    "    prefix_tokens=None\n",
    ")\n",
    "\n",
    "# deleting the hook functions\n",
    "for h in self_attn_hooks + cross_attn_hooks:\n",
    "    h.remove()\n",
    "# end for\n",
    "\n",
    "bart_model.decode(output[0][0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7481d393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0, 'cross'), (1, 'cross'), (2, 'cross'), (3, 'cross'), (4, 'cross'), (5, 'cross'), (6, 'cross'), (7, 'cross'), (8, 'cross'), (9, 'cross'), (10, 'cross'), (11, 'cross')])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff03df52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A new method for detecting hallucinations in large language models has been proposed by researchers.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_model.sample([source_text],\n",
    "                  beam_size=4,\n",
    "                  len_penalty=1.0,\n",
    "                  max_len_b=200,\n",
    "                  min_len=1,\n",
    "                  no_repeat_ngram_size=3,\n",
    "                  extractive_penalty_fct=\"none()\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf54b932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We present a novel approach for detecting hallucinations in large language models.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_source_id = bart_model.encode(source_text)\n",
    "\n",
    "generate_out = bart_model.generate(\n",
    "    tensor_source_id.unsqueeze(0).to(bart_model.device),\n",
    "    beam_size=4,\n",
    "    len_penalty=1.0,\n",
    "    max_len_b=200,\n",
    "    min_len=1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    extractive_penalty_fct=\"none()\"\n",
    ")\n",
    "\n",
    "bart_model.decode(generate_out[0]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96e1a5",
   "metadata": {},
   "source": [
    "# Codebase toward Attention-Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0418e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 250718 06:50:24 2610531437:10] Loading model /workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.xsum/model.pt\n",
      "[I 250718 06:50:38 2610531437:17] Loading done.\n"
     ]
    }
   ],
   "source": [
    "# # with xsum model\n",
    "# PATH_MODEL_FILE = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.xsum')\n",
    "# # with cnn model\n",
    "# # PATH_MODEL_FILE = Path('/workdir/kmitsuzawa/Project/neurips-2025/ConstraintsFact-Dreyer-2023/abstractive-factual-tradeoff/tests/testresources/models/bart.large.cnn')\n",
    "\n",
    "# bart_model = load_model(PATH_MODEL_FILE, PATH_MODEL_FILE / 'model.pt')\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device_obj = torch.device('cuda:0')\n",
    "# else:\n",
    "#     device_obj = torch.device('cpu')\n",
    "# # end if\n",
    "\n",
    "# bart_model = bart_model.to(device_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 0\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 1\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 2\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 3\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 4\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 5\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 6\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 7\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 8\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 9\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 10\n",
      "[D 250718 06:50:38 3983859745:98] Setting attention_hook at a layer at 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_token_source': 170, 'n_token_generated': 10, 'n_layers_decoder': 12, 'n_self_attn_head_decoder': 16, 'n_encoder_attn_header_decoder': 16}\n",
      "(0, 'self') torch.Size([1, 10, 10])\n",
      "(0, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(1, 'self') torch.Size([1, 10, 10])\n",
      "(1, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(2, 'self') torch.Size([1, 10, 10])\n",
      "(2, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(3, 'self') torch.Size([1, 10, 10])\n",
      "(3, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(4, 'self') torch.Size([1, 10, 10])\n",
      "(4, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(5, 'self') torch.Size([1, 10, 10])\n",
      "(5, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(6, 'self') torch.Size([1, 10, 10])\n",
      "(6, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(7, 'self') torch.Size([1, 10, 10])\n",
      "(7, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(8, 'self') torch.Size([1, 10, 10])\n",
      "(8, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(9, 'self') torch.Size([1, 10, 10])\n",
      "(9, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(10, 'self') torch.Size([1, 10, 10])\n",
      "(10, 'cross') torch.Size([16, 1, 10, 170])\n",
      "(11, 'self') torch.Size([1, 10, 10])\n",
      "(11, 'cross') torch.Size([16, 1, 10, 170])\n"
     ]
    }
   ],
   "source": [
    "# from torch.nn import functional as F\n",
    "\n",
    "# from functools import wraps\n",
    "\n",
    "# import fairseq\n",
    "# from fairseq.hub_utils import GeneratorHubInterface\n",
    "# from fairseq.sequence_generator import SequenceGenerator\n",
    "\n",
    "\n",
    "# class GenerateResult(ty.NamedTuple):\n",
    "#     generated_token_ids: torch.Tensor\n",
    "#     generated_text: str\n",
    "#     generation_parameter: ty.Dict\n",
    "#     attention_headers: ty.Dict[ty.Tuple[int, str], torch.Tensor]\n",
    "#     stats: ty.Dict[str, int]\n",
    "\n",
    "\n",
    "# def generate_attention_head_extraction(encoder_decoder_interface: GeneratorHubInterface,\n",
    "#                                        source_token_ids: torch.Tensor,\n",
    "#                                        reference_token_ids: ty.Optional[torch.Tensor] = None,\n",
    "#                                        sampling: bool = False,\n",
    "#                                        beam_size: int = 1,\n",
    "#                                        max_len_a: float = 0.0,\n",
    "#                                        max_len_b: int = 200,\n",
    "#                                        max_len: int = 0,\n",
    "#                                        min_len: int = 1,\n",
    "#                                        normalize_scores=True,\n",
    "#                                        len_penalty: float = 1.0,\n",
    "#                                        unk_penalty: float  =0.0,\n",
    "#                                        temperature: float = 1.0,\n",
    "#                                        no_repeat_ngram_size: int = 0,\n",
    "#                                        random_seed: int = 42,\n",
    "#                                        is_activate_eval_mode: bool = True\n",
    "#                                        ) -> GenerateResult:\n",
    "#     \"\"\"A custom function.\n",
    "    \n",
    "#     NOT Possible:\n",
    "#         `top_p` sampling.\n",
    "#         `top_k` sampling.\n",
    "\n",
    "\n",
    "#     Args:\n",
    "#         reference_token_ids (optinal): used for the teacher-forcing mode.\n",
    "#     \"\"\"\n",
    "#     # input arguments check\n",
    "#     assert len(source_token_ids.shape) == 2, f\"The input tensor must be (batch, n-token). Given -> {source_token_ids.shape}\"\n",
    "#     if reference_token_ids is not None:\n",
    "#         assert len(reference_token_ids.shape) == 2, f\"The `reference_token_ids` tensor must be (batch, n-token). Given -> {reference_token_ids.shape}\"\n",
    "\n",
    "#     assert source_token_ids.shape[0] == 1, f\"This implementation can process only one batch. Given -> {source_token_ids.shape}\"\n",
    "\n",
    "\n",
    "#     if is_activate_eval_mode:\n",
    "#         encoder_decoder_interface.eval()\n",
    "#     # end if\n",
    "\n",
    "\n",
    "#     # Constants\n",
    "#     eos_idx = encoder_decoder_interface.task.source_dictionary.eos()\n",
    "#     # bos_idx = eos_idx  # For BART, BOS and EOS are the same: </s> \n",
    "#     target_ids_start = torch.tensor([[encoder_decoder_interface.model.decoder.dictionary.bos()]]).to(device_obj)\n",
    "\n",
    "#     # ------------------------------------\n",
    "#     # check the `reference_token_ids`. `reference_token_ids` may start from the `target_ids_start`\n",
    "#     if reference_token_ids is not None:\n",
    "#         if all(reference_token_ids[0, 0] == target_ids_start):\n",
    "#             reference_token_ids = reference_token_ids[:, 1:]\n",
    "#         # end if\n",
    "#     # end if\n",
    "\n",
    "#     # ------------------------------------\n",
    "#     # setting hook to extract the attention heads\n",
    "#     attn_data = {}\n",
    "\n",
    "#     def get_attention_hook(layer_idx, attn_type):\n",
    "#         def hook(module, input, output):\n",
    "#             # if output is not None and isinstance(output, tuple) and output[1] is not None:\n",
    "#             #     # print(layer_idx, attn_type)\n",
    "#             #     attn_data.setdefault((layer_idx, attn_type), (output[1].detach().cpu()))\n",
    "            \n",
    "#             # Check output format from MultiheadAttention.forward\n",
    "#             # It can return (attn_output, attn_weights) if `need_weights=True`\n",
    "#             if isinstance(output, tuple):\n",
    "#                 _, attn_weights = output\n",
    "#                 if attn_weights is not None:\n",
    "#                     attn_data[(layer_idx, attn_type)] = attn_weights.detach().cpu()\n",
    "#                 # end if\n",
    "#             # end if\n",
    "#         return hook\n",
    "#     # end def\n",
    "\n",
    "#     # Register hooks for all decoder layers\n",
    "#     self_attn_hooks = []\n",
    "#     cross_attn_hooks = []\n",
    "#     for i, layer in enumerate(encoder_decoder_interface.model.decoder.layers):\n",
    "#         layer.self_attn.register_forward_hook(get_attention_hook(i, 'self'))\n",
    "#         layer.encoder_attn.register_forward_hook(get_attention_hook(i, 'cross'))\n",
    "#         logger.debug(f'Setting attention_hook at a layer at {i}')\n",
    "#     # end for\n",
    "    \n",
    "#     source_token_ids = source_token_ids.to(encoder_decoder_interface.device)\n",
    "    \n",
    "#     if reference_token_ids is not None:\n",
    "#         reference_token_ids = reference_token_ids.to(encoder_decoder_interface.device)\n",
    "#     # end if\n",
    "\n",
    "#     # ------------------------------------\n",
    "#     # setting the internal parameters True for extracting the attention heads.\n",
    "#     # for layer in encoder_decoder_interface.model.decoder.layers:\n",
    "#     #     def wrap_forward(original_forward):\n",
    "#     #         def new_forward(*args, **kwargs):\n",
    "#     #             kwargs['need_attn'] = True\n",
    "#     #             kwargs['need_head_weights'] = True\n",
    "#     #             return original_forward(*args, **kwargs)\n",
    "#     #         return new_forward\n",
    "#     #     layer.forward = wrap_forward(layer.forward)\n",
    "#     # setting the flag to the self-attention layers.\n",
    "#     # overwriting the foward method of `MultiheadAttention`. The default value is False and attn_weights are averaged automatically.\n",
    "#     # https://github.com/facebookresearch/fairseq/blob/d13e14a800bb588e5a77fb4e551f554ff9b24a72/fairseq/modules/multihead_attention.py#L469\n",
    "#     for i, layer in enumerate(encoder_decoder_interface.model.decoder.layers):\n",
    "#         orig_self_attn_forward = layer.self_attn.forward\n",
    "#         orig_encoder_attn_forward = layer.encoder_attn.forward        \n",
    "\n",
    "#         @wraps(orig_self_attn_forward)\n",
    "#         def wrapped_self_attn_forward(*args, **kwargs):\n",
    "#             kwargs['need_weights'] = True\n",
    "#             kwargs['need_head_weights'] = True\n",
    "#             return orig_self_attn_forward(*args, **kwargs)\n",
    "\n",
    "#         @wraps(orig_encoder_attn_forward)\n",
    "#         def wrapped_encoder_attn_forward(*args, **kwargs):\n",
    "#             kwargs['need_weights'] = True\n",
    "#             kwargs['need_head_weights'] = True\n",
    "#             return orig_encoder_attn_forward(*args, **kwargs)\n",
    "\n",
    "#         layer.self_attn.forward = wrapped_self_attn_forward\n",
    "#         layer.encoder_attn.forward = wrapped_encoder_attn_forward        \n",
    "#     # ------------------------------------\n",
    "#     # encoder\n",
    "#     with torch.no_grad():\n",
    "#         encoder_out = encoder_decoder_interface.model.encoder(\n",
    "#             src_tokens=source_token_ids,\n",
    "#             src_lengths=None\n",
    "#         )\n",
    "\n",
    "#     # ------------------------------------\n",
    "\n",
    "#     alignment_layer = 0\n",
    "#     alignment_heads = None  # or specify a head\n",
    "\n",
    "#     # Start decoder input with BOS token\n",
    "#     prev_output_tokens = torch.tensor([[target_ids_start]], dtype=torch.long, device=encoder_decoder_interface.device)\n",
    "\n",
    "#     generated_tokens = [prev_output_tokens.item()]\n",
    "\n",
    "#     # ------------------------------------\n",
    "#     # main loop of the generation.\n",
    "#     i_step = 1\n",
    "\n",
    "#     if max_len == 0:\n",
    "#         _max_len = (len(source_token_ids) * max_len_a) + max_len_b\n",
    "#     else:\n",
    "#         _max_len = max_len\n",
    "#     # end if\n",
    "\n",
    "#     while i_step <= _max_len:\n",
    "#         _decoder_out = encoder_decoder_interface.model.decoder.forward(\n",
    "#             prev_output_tokens=prev_output_tokens,\n",
    "#             encoder_out=encoder_out,\n",
    "#             alignment_layer=alignment_layer,\n",
    "#             alignment_heads=alignment_heads,\n",
    "#             return_all_hiddens=True)\n",
    "\n",
    "#         logits = _decoder_out[0][:, -1, :]  # logits for last token\n",
    "\n",
    "#         if sampling:\n",
    "#             # Apply sampling strategy\n",
    "#             probs = F.softmax(logits / temperature, dim=-1)\n",
    "#             with torch.random.fork_rng(devices=[encoder_decoder_interface.device]):\n",
    "#                 torch.manual_seed(random_seed)\n",
    "#                 torch.cuda.manual_seed_all(random_seed)  # if you are using multi-GPU.\n",
    "#                 __next_token_id = torch.multinomial(probs, num_samples=1)  # stochastic sampling\n",
    "#             # end with\n",
    "#             _next_token = __next_token_id            \n",
    "#         elif reference_token_ids is not None:\n",
    "#             # teacher forcing\n",
    "#             __next_token_id = reference_token_ids[:, (i_step - 1)]\n",
    "#             _next_token = __next_token_id.unsqueeze(0)\n",
    "#         elif temperature == 0.0:\n",
    "#             # greedy search\n",
    "#             __next_token_id = torch.argmax(logits / temperature, dim=-1)  # greedy\n",
    "#             _next_token = torch.stack([__next_token_id])\n",
    "#         else:\n",
    "#             raise Exception('Not defined Parameter combination.')\n",
    "#         # end if\n",
    "        \n",
    "#         # Append to sequence\n",
    "#         prev_output_tokens = torch.cat([prev_output_tokens, _next_token], dim=1)\n",
    "        \n",
    "#         # end if\n",
    "#         generated_tokens.append(__next_token_id.item())\n",
    "\n",
    "#         # end condition\n",
    "#         if __next_token_id.item() == encoder_decoder_interface.task.source_dictionary.eos():\n",
    "#             break\n",
    "#         elif i_step == _max_len:\n",
    "#             break\n",
    "#         elif reference_token_ids is not None and i_step == reference_token_ids.shape[1]:\n",
    "#             break\n",
    "#         else:\n",
    "#             i_step += 1\n",
    "#         # end if\n",
    "#     # end with\n",
    "\n",
    "\n",
    "#     _generation_parameters = dict(\n",
    "#         max_len=_max_len,\n",
    "#         sampling=sampling,\n",
    "#         temperature=temperature\n",
    "#     )\n",
    "\n",
    "#     generated_text = encoder_decoder_interface.decode(torch.tensor(generated_tokens))\n",
    "\n",
    "#     # ------------------------------------\n",
    "#     # stats\n",
    "#     n_layers_decoder = len(encoder_decoder_interface.model.decoder.layers)\n",
    "#     n_self_attn_head_decoder = encoder_decoder_interface.model.decoder.layers[0].self_attn.num_heads\n",
    "#     n_encoder_attn_header_decoder = encoder_decoder_interface.model.decoder.layers[0].encoder_attn.num_heads\n",
    "#     _stats = dict(\n",
    "#         n_token_source=source_token_ids.shape[1],\n",
    "#         n_token_generated=len(generated_tokens) - 1,  # -1 for the BOS token.\n",
    "#         n_layers_decoder=n_layers_decoder,\n",
    "#         n_self_attn_head_decoder=n_self_attn_head_decoder,\n",
    "#         n_encoder_attn_header_decoder=n_encoder_attn_header_decoder\n",
    "#     )\n",
    "\n",
    "#     # ------------------------------------\n",
    "\n",
    "#     generated_obj = GenerateResult(\n",
    "#         generated_token_ids=torch.tensor(generated_tokens),\n",
    "#         generated_text=generated_text,\n",
    "#         generation_parameter=_generation_parameters,\n",
    "#         attention_headers=attn_data,\n",
    "#         stats=_stats\n",
    "#     )\n",
    "\n",
    "#     # deleting the hook functions\n",
    "#     for h in self_attn_hooks + cross_attn_hooks:\n",
    "#         h.remove()\n",
    "#     # end for\n",
    "\n",
    "#     return generated_obj\n",
    "# # end def\n",
    "\n",
    "\n",
    "\n",
    "# def test_teacher_forcing():\n",
    "\n",
    "#     source_text = \"We present a novel approach for detecting hallucinations in large language models (LLMs) by analyzing the probabilistic divergence between prompt and response hiddenstate distributions. Counterintuitively, we find that hallucinated responses exhibit smaller deviations from their prompts compared to grounded responses, suggesting that hallucinations often arise from superficial rephrasing rather than substantive reasoning. Leveraging this insight, we propose a model-intrinsic detection method1 that uses distributional distances as principled hallucination scores, eliminating the need for external knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable kernels that automatically adapt to capture nuanced geometric differences between distributions. Our approach outperforms existing baselines, demonstrating state-of-the-art performance on several benchmarks. The method remains competitive even without kernel training, offering a robust, scalable solution for hallucination detection.\"\n",
    "#     source_ids = bart_model.encode(source_text).unsqueeze(0).to(device_obj)\n",
    "\n",
    "#     target_text_forcing = \"This text introduces a hallucination detection method.\" \n",
    "#     target_token_ids = bart_model.encode(target_text_forcing).unsqueeze(0).to(device_obj)\n",
    "#     # target_token_ids = None\n",
    "\n",
    "#     obj_generated = generate_attention_head_extraction(\n",
    "#         encoder_decoder_interface=bart_model,\n",
    "#         source_token_ids=source_ids,\n",
    "#         reference_token_ids=target_token_ids,\n",
    "#         max_len=10,\n",
    "#         temperature=0.1\n",
    "#     )\n",
    "\n",
    "#     assert target_text_forcing == obj_generated.generated_text\n",
    "#     assert tuple(target_token_ids[0].tolist()) == tuple(obj_generated.generated_token_ids.tolist())\n",
    "# # end def\n",
    "\n",
    "\n",
    "# def test_stochastic_sampling():\n",
    "\n",
    "#     source_text = \"We present a novel approach for detecting hallucinations in large language models (LLMs) by analyzing the probabilistic divergence between prompt and response hiddenstate distributions. Counterintuitively, we find that hallucinated responses exhibit smaller deviations from their prompts compared to grounded responses, suggesting that hallucinations often arise from superficial rephrasing rather than substantive reasoning. Leveraging this insight, we propose a model-intrinsic detection method1 that uses distributional distances as principled hallucination scores, eliminating the need for external knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable kernels that automatically adapt to capture nuanced geometric differences between distributions. Our approach outperforms existing baselines, demonstrating state-of-the-art performance on several benchmarks. The method remains competitive even without kernel training, offering a robust, scalable solution for hallucination detection.\"\n",
    "#     source_ids = bart_model.encode(source_text).unsqueeze(0).to(device_obj)\n",
    "\n",
    "#     target_token_ids = None\n",
    "\n",
    "#     # bart_model.eval()\n",
    "\n",
    "#     obj_generated_1st = generate_attention_head_extraction(\n",
    "#         encoder_decoder_interface=bart_model,\n",
    "#         source_token_ids=source_ids,\n",
    "#         reference_token_ids=target_token_ids,\n",
    "#         max_len=10,\n",
    "#         temperature=0.1,\n",
    "#         sampling=True,\n",
    "#         random_seed=42   \n",
    "#     )\n",
    "#     obj_generated_2nd = generate_attention_head_extraction(\n",
    "#         encoder_decoder_interface=bart_model,\n",
    "#         source_token_ids=source_ids,\n",
    "#         reference_token_ids=target_token_ids,\n",
    "#         max_len=10,\n",
    "#         temperature=0.1,\n",
    "#         sampling=True,\n",
    "#         random_seed=42\n",
    "#     )\n",
    "#     assert obj_generated_1st.generated_token_ids.tolist() == obj_generated_2nd.generated_token_ids.tolist()\n",
    "\n",
    "#     obj_generated_with_05 = generate_attention_head_extraction(\n",
    "#         encoder_decoder_interface=bart_model,\n",
    "#         source_token_ids=source_ids,\n",
    "#         reference_token_ids=target_token_ids,\n",
    "#         max_len=10,\n",
    "#         temperature=1.0,\n",
    "#         sampling=True,\n",
    "#         random_seed=42   \n",
    "#     )\n",
    "#     assert obj_generated_1st.generated_token_ids.tolist() != obj_generated_with_05.generated_token_ids.tolist()\n",
    "# # end def\n",
    "\n",
    "\n",
    "# # test_teacher_forcing()\n",
    "# # test_stochastic_sampling()\n",
    "\n",
    "\n",
    "# source_text = \"We present a novel approach for detecting hallucinations in large language models (LLMs) by analyzing the probabilistic divergence between prompt and response hiddenstate distributions. Counterintuitively, we find that hallucinated responses exhibit smaller deviations from their prompts compared to grounded responses, suggesting that hallucinations often arise from superficial rephrasing rather than substantive reasoning. Leveraging this insight, we propose a model-intrinsic detection method1 that uses distributional distances as principled hallucination scores, eliminating the need for external knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable kernels that automatically adapt to capture nuanced geometric differences between distributions. Our approach outperforms existing baselines, demonstrating state-of-the-art performance on several benchmarks. The method remains competitive even without kernel training, offering a robust, scalable solution for hallucination detection.\"\n",
    "# source_ids = bart_model.encode(source_text).unsqueeze(0).to(device_obj)\n",
    "\n",
    "# obj_generated_1st = generate_attention_head_extraction(\n",
    "#     encoder_decoder_interface=bart_model,\n",
    "#     source_token_ids=source_ids,\n",
    "#     reference_token_ids=None,\n",
    "#     max_len=10,\n",
    "#     temperature=0.1,\n",
    "#     sampling=True,\n",
    "#     random_seed=42   \n",
    "# )\n",
    "\n",
    "# print(obj_generated_1st.stats)\n",
    "\n",
    "# for _key in obj_generated_1st.attention_headers:\n",
    "#     _attention_head = obj_generated_1st.attention_headers[_key]\n",
    "#     print(_key, _attention_head.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98827f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.modules.multihead_attention.MultiheadAttention"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(bart_model.model.decoder.layers[1].encoder_attn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39-Dreyer-2023-2nd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
